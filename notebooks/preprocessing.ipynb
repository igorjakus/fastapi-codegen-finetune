{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe01e8b",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Fine-tuning\n",
    "\n",
    "This notebook prepares the collected FastAPI code examples for model fine-tuning:\n",
    "1. Load and clean the collected examples\n",
    "2. Format data for the tokenizer\n",
    "3. Create train/test/dev split (80/10/10)\n",
    "4. Save processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d644c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igorjakus/projects/fastapi-codegen-finetune/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89205857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1959 examples\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "RAW_DATA_PATH = Path('../data/raw')\n",
    "PROCESSED_DATA_PATH = Path('../data/processed')\n",
    "PROCESSED_DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Load raw data\n",
    "with open(RAW_DATA_PATH / 'fastapi_code_examples.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f'Loaded {len(df)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa40a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of formatted text:\n",
      "--------------------------------------------------\n",
      "<|context|>\n",
      "<|target|>from fastapi import FastAPI\n",
      "<|next|>\n",
      "app = FastAPI()\n",
      "\n",
      "\n",
      "@app.get(\"/\")\n",
      "async def root():\n",
      "    return {\"message\": \"Hello World\"}\n"
     ]
    }
   ],
   "source": [
    "# Format examples for the model\n",
    "def format_example(row: pd.Series) -> str:\n",
    "    \"\"\"Format a single example for the model input\"\"\"\n",
    "    # We use special tokens to help model distinguish parts\n",
    "    return f\"<|context|>{row['context_before']}\\n<|target|>{row['target_line']}\\n<|next|>{row['context_after']}\"\n",
    "\n",
    "df['formatted_text'] = df.apply(format_example, axis=1)\n",
    "\n",
    "# Quick look at formatted example\n",
    "print('Example of formatted text:')\n",
    "print('-' * 50)\n",
    "print(df['formatted_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1d40e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "Train set size: 1567 (80.0%)\n",
      "Test set size:  196 (10.0%)\n",
      "Dev set size:   196 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "# Create train/test/dev split (80/10/10)\n",
    "def create_splits(df: pd.DataFrame, train_size: float = 0.8, test_size: float = 0.1):\n",
    "    \"\"\"Create train/test/dev split\"\"\"\n",
    "    # First split: separate train (80%) from rest (20%)\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, train_size=train_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Second split: divide remaining 20% into test (10%) and dev (10%)\n",
    "    # We need to adjust the test_size to get the right proportion\n",
    "    test_df, dev_df = train_test_split(\n",
    "        temp_df, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_df, test_df, dev_df\n",
    "\n",
    "# Create splits\n",
    "train_df, test_df, dev_df = create_splits(df)\n",
    "\n",
    "print('Dataset splits:')\n",
    "print(f'Train set size: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)')\n",
    "print(f'Test set size:  {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)')\n",
    "print(f'Dev set size:   {len(dev_df)} ({len(dev_df)/len(df)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697a2b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1567/1567 [00:00<00:00, 4346.90 examples/s]\n",
      "Map: 100%|██████████| 1567/1567 [00:00<00:00, 4346.90 examples/s]\n",
      "Map: 100%|██████████| 196/196 [00:00<00:00, 3910.38 examples/s]\n",
      "Map: 100%|██████████| 196/196 [00:00<00:00, 3910.38 examples/s]\n",
      "Map: 100%|██████████| 196/196 [00:00<00:00, 3625.73 examples/s]\n",
      "Map: 100%|██████████| 196/196 [00:00<00:00, 3625.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer (CodeGen)\n",
    "MODEL_NAME = 'Salesforce/codegen-350M-mono'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add special tokens that match CodeGen style\n",
    "special_tokens = ['<|context|>', '<|target|>', '<|next|>']\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "\n",
    "# Set up padding token (using EOS token as padding token, which is common practice)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples: Dict[str, List]) -> Dict[str, List]:\n",
    "    \"\"\"Tokenize examples for the model\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['formatted_text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,  # TODO: adjust based on GPU memory\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Convert to HF datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568007a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1567/1567 [00:00<00:00, 330507.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 196/196 [00:00<00:00, 64366.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1567/1567 [00:00<00:00, 330507.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 196/196 [00:00<00:00, 64366.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 196/196 [00:00<00:00, 88970.08 examples/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed datasets and tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "train_dataset.save_to_disk(PROCESSED_DATA_PATH / 'train')\n",
    "test_dataset.save_to_disk(PROCESSED_DATA_PATH / 'test')\n",
    "dev_dataset.save_to_disk(PROCESSED_DATA_PATH / 'dev')\n",
    "\n",
    "# Also save tokenizer for consistency\n",
    "tokenizer.save_pretrained(PROCESSED_DATA_PATH / 'tokenizer')\n",
    "\n",
    "print('Saved processed datasets and tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60eb9241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset statistics:\n",
      "Train examples: 1567 (80.0%)\n",
      "Test examples:  196 (10.0%)\n",
      "Dev examples:   196 (10.0%)\n",
      "\n",
      "Train split example:\n",
      "==================================================\n",
      "<|context|>from typing import Annotated, Union\n",
      "\n",
      "from fastapi import Depends, FastAPI\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "\n",
      "async def common_parameters(\n",
      "    q: Union[str, None] = None, skip: int = 0, limit: int = 100\n",
      "):\n",
      "    return {\"q\": q, \"skip\": skip, \"limit\": limit}\n",
      "\n",
      "\n",
      "<|target|>@app.get(\"/items/\")\n",
      "<|next|>async def  ...\n",
      "==================================================\n",
      "\n",
      "Test split example:\n",
      "==================================================\n",
      "<|context|>\n",
      "<|target|>from fastapi import FastAPI\n",
      "<|next|>from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "\n",
      "class Item(BaseModel):\n",
      "    name: str\n",
      "    description: str | None = None\n",
      "    price: float\n",
      "    tax: float = 10.5<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|en ...\n",
      "==================================================\n",
      "\n",
      "Dev split example:\n",
      "==================================================\n",
      "<|context|>\n",
      "\n",
      "class UserIn(BaseModel):\n",
      "    username: str\n",
      "    password: str\n",
      "    email: EmailStr\n",
      "    full_name: Union[str, None] = None\n",
      "\n",
      "\n",
      "class UserOut(BaseModel):\n",
      "    username: str\n",
      "    email: EmailStr\n",
      "    full_name: Union[str, None] = None\n",
      "\n",
      "\n",
      "<|target|>@app.post(\"/user/\", response_model=UserOut)\n",
      "<|next ...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Quick validation of splits\n",
    "print('Dataset statistics:')\n",
    "print(f'Train examples: {len(train_dataset)} ({len(train_dataset)/len(df)*100:.1f}%)')\n",
    "print(f'Test examples:  {len(test_dataset)} ({len(test_dataset)/len(df)*100:.1f}%)')\n",
    "print(f'Dev examples:   {len(dev_dataset)} ({len(dev_dataset)/len(df)*100:.1f}%)')\n",
    "\n",
    "# Check a random example from each split\n",
    "for split_name, dataset in [('Train', train_dataset), ('Test', test_dataset), ('Dev', dev_dataset)]:\n",
    "    print(f'\\n{split_name} split example:')\n",
    "    print('=' * 50)\n",
    "    example = dataset[0]\n",
    "    decoded = tokenizer.decode(example['input_ids'])\n",
    "    print(decoded[:300], '...')\n",
    "    print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95e31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
