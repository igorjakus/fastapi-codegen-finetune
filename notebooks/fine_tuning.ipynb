{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7792c93a",
   "metadata": {},
   "source": [
    "# Fine-tuning CodeGen for FastAPI Code Generation using LoRA\n",
    "\n",
    "This notebook performs parameter-efficient fine-tuning of the CodeGen-350M model using LoRA (Low-Rank Adaptation).\n",
    "\n",
    "## Google Colab Setup\n",
    "First, we need to mount Google Drive, check GPU availability and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32276c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Install required packages\n",
    "!pip install torch transformers datasets numpy peft accelerate\n",
    "\n",
    "# Setup paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project paths\n",
    "BASE_PATH = Path('/content/drive/MyDrive/fastapi-codegen')\n",
    "PROCESSED_DATA_PATH = BASE_PATH / 'data/processed'\n",
    "MODEL_PATH = BASE_PATH / 'models'\n",
    "\n",
    "# Create directories\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "(MODEL_PATH / 'checkpoints').mkdir(exist_ok=True)\n",
    "\n",
    "print(f'Project directory: {BASE_PATH}')\n",
    "print(f'Processed data directory: {PROCESSED_DATA_PATH}')\n",
    "print(f'Model directory: {MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Clear GPU memory if needed\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b189105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "# PROCESSED_DATA_PATH = Path('../data/processed')\n",
    "# MODEL_PATH = Path('../models')\n",
    "# MODEL_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_from_disk(PROCESSED_DATA_PATH / 'train')\n",
    "test_dataset = load_from_disk(PROCESSED_DATA_PATH / 'test')\n",
    "dev_dataset = load_from_disk(PROCESSED_DATA_PATH / 'dev')\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PROCESSED_DATA_PATH / 'tokenizer')\n",
    "\n",
    "print(f'Train size: {len(train_dataset)}')\n",
    "print(f'Test size: {len(test_dataset)}')\n",
    "print(f'Dev size: {len(dev_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4743130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "MODEL_NAME = 'Salesforce/codegen-350M-mono'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map='auto',\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank of update matrices\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\n",
    "        \"wte\",  # token embeddings\n",
    "        \"c_attn\",  # attention weights\n",
    "        \"c_proj\",  # projection layers\n",
    "        \"c_fc\"  # feed-forward layers\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Add LoRA adapters to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Print trainable parameters info\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af60b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for LoRA\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(MODEL_PATH / 'checkpoints'),\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    learning_rate=5e-4,  # Higher learning rate for LoRA\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print('Starting training...')\n",
    "trainer.train()\n",
    "\n",
    "print('\\nTraining completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print('Test results:', test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and LoRA weights\n",
    "model.save_pretrained(str(MODEL_PATH / 'final'))\n",
    "print('Saved final model with LoRA weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
